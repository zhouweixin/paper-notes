## <center>Chat More: Deepening and Widening the Chatting Topic via A Deep Model</center>
---

**1. 研究背景是什么？**

    Dialog system:
        1) task-oriented: vertical domains
        2) non-task-oriented: open-domain topics

    Implement methods:
        1) Rule-based: restrict diversity
        2) Retrieval-based: depend repository
        3) Generation-based: more flexible
            a) Single-turn: neglect historical conversation
            b) Multi-turn
    
The past decade has witnessed the boom of human-machine interactions, particularly via dialog systems. Dialog systems  have been widely used in a variety of applications, spanning from entertainment and knowledge sharing, to customer services. Dialog systems can be divided into task-oriented and non-task-oriented categories. The former studies vertical domains. The latter studies open-domain topics.

**2. 在上述背景下研究什么问题？**

    Response generation in open-domain multi-turn dialog systems

In this paper, authors study the task of response generation in open-domain multi-turn dialog systems. For example,

person A: There is a heavy rain today.
person B: The umbrella is totally useless.
person A: The rain is really heavy. I got wet in the afternoon and caught a cold at night.
person B: You should take some hot tea and get a good sleep.

**3. 针对这个问题，已有研究有什么不足？**
    
    1) Without elaborated distinct, incorporates noises
    2) No more attractive and meaningful
    3) Dull response

**4. 解决这些问题有什么挑战？**
    
    1) How to identify the relevant words to eﬀectively guide the response generation?
    2) How to avoid dull responses and generate responses that are not only relevant but also capable of deepening and widening the dialog topics?
    3) How to construct a large-scale dataset for generation-based models?

**5. 本文有哪些创新点？**

    1) Study the context-aware dialog systems
    2) Deepen and widen the chatting topic

**6. 本文的模型？**

![模型结构图](./resource/images/chat-more-figure-1.png)

**1) Keyword Extraction**

    TF-IDF:
    a) Retain nouns, verbs and adjectives, Remove other words
    b) A session as a document
    c) A word as a term

**2) Global Channel**

        GRU: encode the given context into a vector
![GRU](./resource/images/chat-more-figure-2.png)

**3) Wide Channel**

        Attention-based RNN: predict keywords to extend topics

![Attention-RNN](./resource/images/chat-more-figure-3.png)

**4) Deep Channel**

    A MLP model: choose the useful keywords to deepen the topic of interest

![MLP](./resource/images/chat-more-figure-5.png)

**5) Decoder**

    Attention-based RNN

![Attention-RNN](./resource/images/chat-more-figure-4.png)

**6) Loss Function**

![Loss Function](./resource/images/chat-more-figure-6.png)

**7. 本文做出了哪些贡献？**

    1) Separate the topic-related keywords from the irrelevant ones, avoid dull responses
    2) Hybrid RNN and DNN model to deepen and widen the chatting topic firstly
    3) Construct a dataset of multi-turn dialogs in the open domain and release the data, code and parameters

**8. 我们何去何从，如何进一步改进**

    shed light on the logical and semantic consistency between the responses and the historical contexts

